# 激活函数

## 激活函数的作用

神经网络是线性的，无法解决非线性的问题，加入激活函数就是给模型引入非线性能力。
一个单层网络：y=Wx+b，这是线性变换。
如果多层堆叠的话，y=W3（W2（W1x+b1）+b2）+b3，看上去很复杂，但是线性变换的叠加仍然是线性变换。
y=(W3W2W1)x+(W3W2b1+ W3b2+b3)。矩阵乘法仍然是线性的，加法也是线性的。y=W`x+b`，所以多层线性网络=一个大型线性模型。
所以没有激活函数的神经网络，本质上就是一个线性模型。
无论你堆多少层，它的表达能力不会增强，仍然只能学到线性的映射。在线性代数中，一个线性变换有放大、缩小、平移、旋转、投影。这里说的线性映射指的也是矩阵的线性操作。
需要注意的是这里面的y和x和b都是向量，而W是矩阵。
放大和缩小是一样的。举个例子。

$$
S=
\begin{pmatrix}
k1 & 0\\
0 & k2\\
\end{pmatrix}
$$

当k1=k2>1的时候是放大。当0<k1=k2<1的时候是缩小。
当k1=1，k2=-1的时候是沿x轴翻转。
当k1=-1，k2=1的时候是沿y轴翻转。

$$
R=
\begin{pmatrix}
\cos \theta & -\sin \theta\\
\sin \theta & \cos \theta\\
\end{pmatrix}
$$

这个矩阵就是旋转矩阵。θ的大小就是旋转角度的大小。
投影是将一个向量“影射”到某个方向或者某个子空间。
投影到 x 轴：

$$
S=
\begin {pmatrix}
1 & 0\\
0 & 0\\
\end {pmatrix}
$$

投影道某个单位向量u，若u是单位向量，则投影矩阵为:$P=uu^{\rm T}$。
在几何意义里，“平移（Translation）”就是：x——>x+t
y=W′x+b′ 不是“平移”，它是“线性变换 + 平移 = 仿射变换。

线性模型只能画直线，世界是由曲线构成的，所以必须有激活函数来生成非线性。
激活函数（ReLU、Sigmoid、Tanh 等）有一个关键特性：它是非线性的！非线性的结构可以破坏线性叠加规则。
Sigmoid和tanh的特点是将输出限制在（0，1）和（-1，1）之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。
ReLU适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。
LSTM（Long Short-Term Memory）长短期记忆网络，LSTM 是为了解决 RNN 的“记不住”和“梯度消失”问题而设计的特殊循环神经网络。不过现在LSTM基本已经被transformer淘汰了。

## 梯度爆炸和梯度消失：

模型中的梯度爆炸和梯度消失问题：
    激活函数导致的梯度消失，像sigmoid和tanh都会导致梯度消失。
    矩阵连乘也会导致梯度消失，这个原因导致的梯度消失无法通过更换激活函数来避免。直观的说就是再反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸。
如何解决梯度爆炸和梯度消失的问题：
    上述第一个问题只需要用像ReLU这种激活函数就可以解决。
    矩阵连乘也会导致梯度消失。这个原因导致的梯度消失无法通过更换激活函数来避免。直观的说就是在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时候，就会出现梯度爆炸。

##### 梯度的定义

梯度本质上就是导数。一个因变量y有一个自变量x，这时候就会有一个导数x`。一个因变量y有十个自变量x,这时候就会有10个导数。

而对于我们深度学习来说，因变量和自变量都变成了矩阵了。这个时候梯度也就变成了矩阵了。每一个参数（权重矩阵、偏置向量）都有自己的梯度矩阵。  **梯度的维度和参数完全一致。**

接下来将为什么sigmoid和tanh会导致梯度消失，而ReLU不会导致梯度消失。

### Sigmoid

Sigmoid函数公式：

$$
\sigma (z)=\frac {1} {1+e^{-z}}
$$

它的导数公式是

$$
\sigma'(z)=\sigma(z)(1-\sigma(z))
$$

![](C:\Users\15175\AppData\Roaming\marktext\images\2025-12-04-19-48-16-image.png)

优点：平滑、易于求导。取值范围是（0，1），可用于直接用于求概率值的问题或者分类问题。

缺点：计算量大，包含幂运算，以及除法运算。sigmoid的导数的取值范围是[0,0.25],最大值都是小于1的。反向传播的时候是链式传导，经过几次相乘之后就很容易出现梯度消失的问题。sigmoid的输出的均值不是0，均值偏正，大约在0.5左右。每一层的输出都偏正，因此向下一层接受到偏置偏向正的输入，层数一深，偏置累积，网络的数据分布就会越来越偏移了。

### Tanh

Tanh的函数公式为：

$$
tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}=\frac{2}{1+e^{-2z}}-1
$$

从上述公式的第二行可以看出，tanh函数可以由sigmoid函数平移和拉伸得到。tanh的函数的取值范围是（-1，1）。

$$
tanh(x)=2\sigma(2x)-1
$$

${\sigma(2x)}$代表输入乘2了。拉伸了水平方向。

$2\sigma(2x)$代表了在上面的基础上输出乘2了，拉伸了竖直方向。

$2\sigma(2x)-1$ 代表了向下平移了1。

导数公式：



$$
tanh(x)'=\frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2}
=1-(tanh(x))^2
$$

![](C:\Users\15175\AppData\Roaming\marktext\images\2025-12-08-19-31-31-image.png)



tanh函数可以理解为是基于sigmoid函数的一种改进的激活函数，所以对于sigmoid函数的缺点，它能够解决一部分。但是tanh函数仍然有不少的缺点。

tanh函数的输出范围是（-1，1），解决了sigmoid函数输出的均值不是0的问题。

tanh的导数取值范围是（0，1），可以看出其在反向传播的“链式传导”过程中的梯度消失问题要比sigmoid函数好一些，但是其依然存在着梯度消失问题。

为什么会说tanh函数在反向传播的“链式传导”过程中的梯度消失问题要比sigmoid函数好一些呢？因为两者的导数范围，tanh的导数范围就更大一些。



## ReLU系列

### ReLU激活函数

ReLU为修正线性单元函数，公式如下：

$$
ReLU(z)=
\begin{cases}
0 &  \text{if z}\le0 \\
z & \text{if z}> 0 \\

\end{cases}
$$

导数是：

$$
ReLU'(z)=
\begin{cases}
0 & \text{if z} \le 0\\
1 & \text{if z} > 0\\


\end{cases}
$$



相比于sigmoid函数和tanh函数，ReLU激活函数的优缺点如下：

1.当$z>0$时，ReLU激活函数的导数恒为1，这就避免了梯度消失的问题。

2.计算复杂度降低，没有幂运算，只需要一个阈值函数就能得到其导数。

3.使用ReLU作为激活函数，模型的收敛速度比sigmoid和tanh函数更快。

4.当z<0时，激活函数的导数恒为常数0。

        有利的方面：在深度学习中，目标是从大量数据中学习到关键特征，也就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU 激活函数中将 `z<0`的部分置为0，就是产生稀疏矩阵的过程。

        坏的方面：将 `z<0`的部分梯度直接置为0会导致 Dead ReLU Problem(神经元坏死现象)。**可能会导致部分神经元不再对输入数据做响应，无论输入什么数据，该部分神经元的参数都不会被更新**。（这个问题是一个非常严重的问题，后续不少工作都是在解决这个问题）

5.ReLU有可能会导致梯度爆炸的问题，解决的方法是梯度截断。梯度截断的常用方法就是之后会提到的归一化。

6.ReLU的输出不是0的均值。这个和sigmoid类似，会导致数据偏移。

注：**在这里要注意我们一会是分析导数的影响，一会儿是分析函数本身的影响。像数据偏移就是函数本身的影响。而梯度爆炸和梯度消失是函数导数的影响。****



### Leaky ReLU

$$
\text{LeakyReLU}(z) =
\begin{cases}
0.01z & \text{if } z \le 0 \\
z & \text{if } z > 0
\end{cases}

$$



为了解决ReLU的神经元坏死现象，即在$z\le0$ 的时候，激活函数的值是0。Leaky ReLU在z>0的时候，函数与ReLU相同。在$z\le0$ 的时候，有一个非常小的斜率0.01。

![](C:\Users\15175\AppData\Roaming\marktext\images\2025-12-09-19-46-36-image.png)

这个激活函数不太常用。



### GELU

GELU（高斯误差线性单元）是目前Transformer等大模型中最常用的激活函数了。它是RELU激活函数和sigmoid激活函数的结合体。既有ReLU的保留正数的思想，又有sigmoid的概率平滑的思想。

$$
GELU(x)=x·\Phi(x)\\
$$

$$
\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} e^{-\frac{t^2}{2}}\, dt
$$

标准正态分布的概率密度函数和累积分布函数的图像。

![](C:\Users\15175\AppData\Roaming\marktext\images\2025-12-09-20-01-53-image.png)

先分析$\Phi(x)$ 有哪些性质呢？

1.在x很小的时候，值趋近于0。

2.在x很大的时候，值趋近于1。

而$x·\Phi(x)$ 有哪些特性呢？

1.在输入x是一个较大的正值的时候，GELU(x)是一个接近于1的恒等映射。

2.在输入x是一个较小的负值的时候，GELU(x)是一个接近于0的置零映射。

$$
GELU'(x)=\Phi(x)+\frac{x}{\sqrt{2\pi}}e^{-x^2/2}
$$

相比于ReLU激活函数，GELU的激活函数的梯度不会在0附件突然断开。

![](C:\Users\15175\AppData\Roaming\marktext\images\2025-12-09-20-23-41-Figure_1.png)

有一种方法是精确求解

激活函数（包括 GELU）是神经网络每一层中必要的一步运算，它直接参与前向计算和反向传播。**每一层前向传播都要计算激活函数**  ****反向传播也必须计算 GELU 的导数****

激活函数 **不是预先算好存起来的常量**  而是对每一个神经元的输入 z 进行实时计算

需要对GELU进行加速，因为它的公式当中包含指数、积分、误差函数等等。

比较常用的加速计算的方法是**不精确求解，而是求解其近似值。**



$$
GELU=0.5*x \left (1+tanh \left [\sqrt{\frac{2}{\pi}}+0.044715x^3 \right]\right)
$$

### GLU

核心思想是用一道门来控制输入信息的通过比例。

数学公式是

$$
GLU(a,b)=a \otimes \sigma(b)
$$

a是主信息通道，b是门控信号，$\sigma$是sigmoid激活函数，$\otimes$ 是逐元素相乘。

y=x[:,:d]*sigmoid(x[:,d:])   注：要看清楚，这里面分成了两部分，一部分在d前面，一部分在d后面

GLU ≈ “把输入拆成信息和开关，让开关去控制信息流”

**GLU 是一种替代普通激活函数的结构**  它是一种 **门控式激活机制**。它就位于之前激活函数所处的位置。

普通的激活函数：**x → 激活函数 → y**

GLU:   **x → Linear → [a, b] → a × sigmoid(b) → y***


