# 损失函数

损失函数用于评估模型的预测值与实际值之间的差异程度。在训练过程中，损失函数作为优化的目标，通过最小化损失函数的值来调整模型参数，从而提高模型的预测准确性。

具体来说，损失函数将模型的预测输出（例如，一个分类任务中类别概率分布）与真实标签（或真实值）进行比较，并计算出一个表示差异的数值。这个数值越大，表示模型的预测越不准确；数值越小，表示模型的预测越接近真实情况。

接下来，介绍几个常用的损失函数。

- L1 Loss损失函数：通常用于回归任务中。

- NLL Loss损失函数：在多分类问题中广泛应用。

- MSE Loss损失函数：在多种回归任务中表现出色。

- BCE Loss损失函数：广泛应用于各类二分类任务中。

- CrossEntropyLoss交叉熵损失函数：广泛应用于多分类问题中。

## 1.L1 Loss损失函数

L1 Loss损失函数，也被称为**平均绝对误差**，是深度学习和机器学习中常用的一种损失函数，特别是在回归任务中。

### 1.1定义

L1Loss计算的模型预测值f(x)与真实值y之间差的绝对值的平均值。其数学表达式为：

$$
text_L1Loss=\frac {1}{n} \sum_{1}^n|y_i-f(x_i)|
$$

其中，n是样本数量，$y_i$ 是第$i$ 个样本的真实值，$f(x_i)$ 是模型对第$i$ 个样本的预测值。

注意区分L1Loss和L1正则化的区别。

### 1.2优缺点

优点：

- 1.稳定性：L1Loss对于所有输入值都有稳定的梯度，不会导致梯度爆炸问题，因此具有较为稳健的解。

- 2.鲁棒性：L1Loss对于噪声和异常值(离群点)具有相对较强的鲁棒性，因为它不会因个别异常值而产生过大的误差。

- 3.很多教材和博客会说L1损失会有稀疏性，但是实际上这是错误的。这是搞混了L1损失和L1正则化了。L1正则化会产生稀疏的特征矩阵，很多无用或者影响较小的特征权重会被置为0。但是L1loss不会产生稀疏权重，因为L1 Loss 的绝对值作用在“预测误差”上，而不是作用在“参数”上。

缺点：

- 1.不可导性：在0点处，L1loss的梯度不存在，因为绝对值函数在0点不可导。L1 Loss 在 0 点不可导，  并不是“优化失败”，  而是“已经到达目标”的数学表现。
- 2.收敛速度，与L2loss相比，L1loss损失在误差较大时，其梯度是恒定的。在离最优解很远的时候，即误差较大的时候，L2loss的梯度很大，下降很快。但是L1的loss仍然是$\pm1$，下降相对较慢。但是当接近最优解的时候，即误差很小的时候，L2的误差接近0了，所以它的梯度也接近0了，这个时候更新的步长自然变小了，可以平滑稳定的收敛。而L1loss虽然很小了，但是梯度仍然是$\pm1$，所以更新的步长不会自动变小，所以更容易在最优点附件来回跳动。所以L1收敛更慢，但并不是因为在接近最优点的时候它的梯度更小了，而是因为它来回跳动。

## 2.MSE Loss损失函数

MSE Loss损失函数就是L2 loss损失函数。MSELoss损失函数，全称为均方误差损失函数，是深度学习中常用的一种回归损失函数。

### 2.1 定义与原理

MSELoss通过计算预测值与真实值之间的差的平方的平均值来衡量模型的性能。具体来说，对于每个样本，它计算预测值与真是值之差的平方，然后对所有样本的平方误差求和并取平均，得到最终的损失值。这种损失函数旨在通过最小化预测值与真实值之间的差异来优化模型参数，从而提高模型的预测准确性。

对于单个样本，假设预测值为$\hat(y)$，真是值为$y$，则该样本的均方误差为$(\hat y -y)^2$。对于包含$n$个样本的数据集，MSELoss的计算公式为：

$$
textMSELoss=\frac {1}{n} \sum_{i=1}^n(\hat y_i-y_i)^2
$$

### 2.2 优点与注意

优点：

- 1.优化直观：MSE结果是一个平滑且凸的优化直观，这有助于使用基于梯度的算法（如梯度下降）进行高效优化。

- 2.唯一极小值：MSE具有唯一的全局极小值，这简化了优化过程，并在某些情况下可以获得解析解。

- 3.可微性：MSE在任何地方都是可微的，这使得在训练过程中可以使用基于梯度的优化方法。

- 4.广泛适用性：MSE是可回归问题的标准且广泛使用的损失函数，适用于预测连续的数值。

注意：

- 1.对异常值敏感：由于MSE计算的是误差的平方，因此它对异常值非常敏感。当数据集中存在极端值时，这些异常值可能会对损失值产生不成比例的影响，从而导致模型性能下降。

- 2.非直观的尺度：MSE的尺度收到平方差的影响，这可能导致其解释性较差。特别是在与原始数据的尺度相比时，MSE可能难以直观地范颖模型预测的准确性。

#### 2.3应用

MSELoss在多种回归任务中表现出色，包括但不限于房价预测、股票价格预测、气温预测等。在这些任务中，模型需要输出一个连续的数值预测结果，而MSELoss能够有效地评估模型预测结果与实际值之间的差异，并指导模型的优化方向。

## 3.NLL Loss损失函数

NLLLoss损失函数是**负对数似然损失**，是深度学习中常用的一种损失函数，尤其在处理多分类问题时表现出色。

#### 3.1 定义与原理

NLLLoss衡量的是模型的预测概率分布与真实标签之间的差异的损失。在PyTorch等深度学习框架中，它通常用于多分类任务。具体来说，NLL Loss计算的是对数概率的负值与真实标签之间的交叉熵损失。这样做的目的是通过最小化损失来优化模型参数，使得模型的预测结果更加接近真实标签。

对于单个样本（分类）：

$$
\mathcal{L}_{NLL}=-log p(y_{true}|x)
$$

- 似然是指在给定参数/模型的情况下，数据出现的概率。

- 我们希望模型让真实标签出现的概率$p(y_{true}|x)$越大越好。

- 训练通常是最小化损失，所以把“最大化概率”变成了“最小化负对数概率”

为什么要用对数呢？

- 使用对数log可以把连乘变成连加。

- 概率很小时，$-\log p$会变得很大，这样惩罚强。要记住这里的$p$是模型让真实标签出现的概率，肯定是越大越好，越小越不好。

这里有一个困惑，既然NLL Loss是在计算交叉熵损失，为什么和KL散度的笔记当中的交叉熵损失是不一样的呢？接下来解答这个问题。

**KL散度当中的交叉熵损失：**

$$
\begin{matrix}
H(p,q)=\mathbb{E}_{x \sim p} \left[ -\log q(x) \right]
=
\sum_x p(x)(-\log q(x))



\end{matrix}
$$

这里：

$p(x)$：真实分布

$q(x)$：模型分布

**NLL Loss 当中对单个样本的交叉熵损失:**

$$
\mathcal{L}_{NLL}=-log p(y_{true}|x)
$$

**关键桥梁是NLL Loss解决的是分类问题，而分类中的真实分布**$p(x)$**如下所示：**

在监督分类中，真实标签都是确定的，所以真实分布$p(y|x)$是one-hot分布。

$$
p(y=k|x)=
\begin{cases}
1 & k=y_{true}\\
0 & k\ne y_{true}
\end{cases}
$$

把这个one-hot的p待会KL的交叉熵定义里面就会得到



$$
\begin{matrix}
H(p,q)
&=\sum_x p(x)(-\log q(x))\\
&=1·(-\log q(y_{true}|x))+0·(其他项)\\
&=-\log q(y_{true}|x)
\end{matrix}
$$

这个时候KL散度当中的交叉熵损失和NLLLoss当中的交叉熵损失对应上了。

这个时候可能又要小伙伴问了，为什么一会儿是$p(x)$ 一会是$p(y|x)$呢？

主要是大家用的参数不同，其实如果都放到输出结果来说的话，KL散度当中就应该用$p(y_{true}|x)$了，因为大家有的时候默认条件在$x$上就会把$x$省略。

$p(y_{true}|x)$ 代表在已知$x$输入的情况下，$y_{true}$的概率。就是已知输入的情况下，真实标签的概率。



## 4.BCE Loss 损失函数

BCELoss损失函数，全称为二元交叉熵损失函数，是深度学习中常用于二分类问题的一种损失函数。

#### 4.1 定义与原理

BCELoss通过计算模型预测的概率分布与实际标签之间的交叉熵损失来评估模型的性能。在二分类问题中，每个样本的真实标签是0或1，而模型输出的是一个介于0和1之间的概率值，表示该样本属于正类的概率。BCELoss通过比较这两个值之间的差异，为模型提供一个损失值，该值越大表示模型预测越不准确。

对于单个样本，BCELoss的数学公式为：

$$
textBCELoss=-(y\log(p)+(1-y)\log(1-p))
$$

其中$y$是实际标签（0或者1），$p$是模型输出的概率值（预测为正类的概率），$\log$是自然对数。

- 当 `y=1` 时，损失函数简化为 $-\log p$，此时如果 *`p`* 越接近 1，则损失越小；
- 当 `y=0` 时，损失函数简化为$ −log(1−p)$，此时如果 *`p`* 越接近 0，则损失越小。
  
  这样就把样本标签为0或者为1的给统一起来了。对于一批样本，BCELoss通常是对所有样本的BCELoss求和后取平均值。
  
  

#### 4.2 优点与注意

优点：

1.直观性：BCELoss能够直观地反映模型预测的概率分布与实际标签之间的差异，从而指导模型的优化方向。

2.鲁棒性：在二分类问题中，BCELoss对正负样本的损失形式是对称且明确的，使得模型在训练过程中能够同时关注到正负样本的分类情况。



注意：

1.输入要求：在使用BCELoss时，需要注意模型输出的概率值应该经过Sigmoid函数或其他适当的激活函数处理，以确保其值在0和1之间。

2.标签要求：BCELoss要求真实标签必须是二值化的（0或者1），而不是其他形式的标签，所以必须是二分类问题。

3.**数值稳定性**：在计算BCELoss时，需要注意数值稳定性问题。例如，当预测概率 *`p`* 非常接近0或1时，$log()$ 或 的值可能会变得非常大或非常小，导致计算过程中出现数值问题。为了避免这种情况，可以对 *`p`* 进行一些**平滑处理**（如添加一个小的正数 *`ϵ`* 到 *`p`* 和 `1−p` 中）。

#### 4.3应用

BCELoss广泛应用于各类**二分类任务中**，如文本情感分析（积极/消极）、垃圾邮件检测（垃圾邮件/非垃圾邮件）、病患诊断（患病/未患病）等。在这些任务中，模型需要输出一个二分类的概率预测结果，而BCELoss能够有效地评估模型预测的准确性，并指导模型的优化方向



## 5.CrossEntroyLoss损失函数

CrossEntroyLoss损失函数，也称为交叉熵损失函数，是深度学习中常用于分类问题的一种常用的损失函数。它衡量的是模型预测的概率分布与真实标签的概率分布之间的差异。

#### 5.1 CrossEntroyLoss损失函数

交叉熵损失函数通过比较模型对每个类别的预测概率和真实的标签（通常是独热编码形式）来计算损失。如果模型对某个样本的概率分布与真实标签越接近，则交叉熵损失越小；反之，损失越大。

对于多分类问题，假设有$C$个类别，对于每个样本，交叉熵损失的计算公式如下：

$$
textCrossEntroyLoss=-\sum_{c=1}^cy_c \log(p_c)
$$

其中，$y_c$是样本的真实标签中的第$c$ 类的值（在独热编码中，只有一个元素为1，其余为0），$p_c$是模型预测的第$c$ 类的概率。

将独热编码带入其中，此时只有$y_c$是1，其余都是0，可以得到：

$$
textCrossEntroyLoss=- \log(p_c)
$$

这个时候和NLLLoss是相同的。

上面说了在数学上面是一致的，不过在实际的应用计算中不是一致的了。

下面讲一讲是如何应用的。

**A. logits（模型原始输出）**

模型最后一层一般输出一个长度为 C 的向量：

$z=(z_1​,…,z_C​)$z=(z1​,…,zC​)

它们叫 **logits**，不是概率、也不是 log 概率。

**B.LogSoftmax：把 logits 变成 log-probabilities**

输出是一个向量：$logp=(logp_1​,…,logp_C​)$

**C. NLLLoss：从 log-probabilities 里取真实类那一项并取负号**

**它要求输入已经是 $\log p$**。

**把这三步串起来,封装起来，就是 CrossEntropyLoss**

**所以 CE 把 LogSoftmax + NLLLoss 打包，是为了稳定且高效**

#### 5.2 优点与注意

**优点**：

1. **直观性**：交叉熵损失能够**直观地反映**模型预测的概率分布与真实标签之间的差异。
2. **易于优化**：由于交叉熵损失函数是凸函数（在模型输出为softmax概率的情况下），因此可以使用**梯度下降**等优化算法来有效地最小化损失。
3. **鲁棒性**：交叉熵损失对预测概率的**微小变化敏感**，这有助于模型在训练过程中更准确地逼近真实标签。

**注意**：

1. **输入要求**：在使用交叉熵损失函数时，需要确保**模型输出的是概率值**（通常通过softmax函数进行转换），而真实标签是独热编码形式的。
2. **数值稳定性**：当预测概率接近0时，log(*p*) 的值会趋于负无穷，这可能导致数值问题。为了解决这个问题，可以在计算对数之前对预测概率进行**平滑处理**（例如，添加一个小的正数 *ϵ* 到预测概率中）。
   
   $logSoftmax(z)=ϵ−log \sum_k e^{zk}$​
3. **权重平衡**：在处理类别不平衡的数据集时，可以为不同类别的损失分配不同的权重，以改善模型的性能。

#### 5.3 应用

交叉熵损失函数广泛应用于**多分类问题**中，如图像分类、文本分类等。在这些任务中，模型需要输出每个类别的概率预测，而交叉熵损失函数能够有效地评估模型预测的准确性，并指导模型的优化方向。
