# 正则化

1.正则化常用于缓解模型过拟合。过拟合发生的原因是模型的容量过大，而正则化可以对模型施加某些限制，从而降低模型的有效容量。

2.目前有多种正则化策略。

    有些正则化策略是向模型添加额外的约束，如增加对参数的限制。这是对参数的硬约束。

    有些正则化策略是向目标函数增加额外项，这是对参数的软约束。

3.正则化策略代表了某种先验知识，即：倾向于选择简单的模型。

4.在深度学习中，大多数正则化策略都是基于对参数进行正则化。正则化以偏差的增加来换取方差的减少，而一个有效的正则化能显著降低方差，并且不会过度增加偏差。

5.在深度学习的实际应用中，不要因为害怕过拟合而采用一个小模型，推荐采用一个大模型并使用正则化。

下面讲一讲偏差和方差的区别：

偏差：模型的表达能力是否不足，导致它学不到真实规律的系统性误差。

方差：模型对训练数据的波动是否过于敏感，导致它对不同数据学得不稳定。

    偏差反映的是，如果模型本身就不包含真实函数，那你怎么训练，偏差都下不来。

方差描述的是：模型对数据扰动有多敏感。如果一个模型：训练集稍微一变，学到的函数就大变样了，那就是方差大。

**正则化以偏差的增加来换取方差的减小。**

适度的正则化去掉的是噪声的自由度，而不是关键信号的自由度。

在神经网络中，偏差大的典型表现是：训练集loss就很高，模型怎么调整都学不好。

而方差大的典型表现是：训练集loss很低，验证集loss很高。结果对随机种子极其敏感。

## 1 参数约束正则化：L1和L2正则化

参数约束正则化主要是指L1和L2正则化：

L1正则化：给每个权重减去一个与$sign(\theta_i)$同符号的常数因子

L2正则化（权重衰减）：给每个权重值乘上一个常数因子，线性的缩放每个权重值。

## 1.1 L1正则化

正则化项为$\Omega(\theta)=||\theta||_{1}=\sum_{i=1}^{d}\left|\theta_{i}\right|$

其中$\theta=(\theta_1,\theta_2,\cdots,\theta_d)$ 是模型的参数向量

$|\theta_i|$是第$i$ 个参数的绝对值

把所有参数的绝对值加起来是L1范数

所以**L1正则化=惩罚所有参数“绝对值的总大小”**

**L1正则化在loss里面是怎么使用的呢？**

真实训练的时候，优化目标是：

$$
L(\theta)=L_{task}(\theta)+\lambda \sum_{i=1}^d|\theta_i|
$$

$L_{task}$是任务损失（分类任务、回归任务）

$\lambda$ 是正则化强度，用来控制惩罚有多大

**既要把任务做好，又要让参数“尽量少用/尽量小。本质上L1正则化鼓励参数变成0，而不是只变小。这是它和L2的本质区别。”**

#### 从梯度角度看L1

对于单个参数：

$$
\frac{\partial}{\partial \theta_i}|\theta_i|=
\begin{cases}
+1 & \theta_i >0\\
-1 & \theta_i<0\\
不可导 & \theta_i=0

\end{cases}
$$

这代表梯度大小是常数，1或者-1，和参数大小是无关的。

梯度的更新规则是：新参数=旧参数-学习率×梯度

即：沿着损失下降最快的方向，走一小步。

梯度指向的是“损失上升最快的方向。”

用一个一维的简单例子来解释梯度在损失当中代表了什么。

$$
L(\theta)=\theta^2
$$

梯度为：$\frac {d\mathcal{L}}{d\theta}=2\theta$

$\theta>0$:梯度为正，所以向右走损失增大更快。

$\theta<0$:梯度为负，所以向左走损失增大更快。



## 1.2 L2正则化

正则化项为$\Omega(\theta)=\frac{1}{2}||\theta||_2^2$

其中：$||\theta||_2^2=\sum_{i=1}^d{\theta_i}^2$

其中$\theta=(\theta_1,\theta_2,\cdots,\theta_d) $是模型的参数向量

$||\theta||_2^2$ 代表所有参数的平方的和

$\frac{1}{2}$ 一个数学上的便利系数，为了求导时得到的系数为1。

**正则化能够使参数$\theta$ 的方差更接近0**

目标函数：

$$
\mathcal{L}(\theta)=\mathcal{L}_{task}(\theta)+\lambda \frac{1}{2}||\theta||_2^2
$$

梯度就是：

$$
\frac{\partial \mathcal{L}}{\partial \theta}=
\frac{\partial \mathcal{L_{task}}}{\partial \theta}+
\lambda \theta
$$

正则化项为$\Omega(\theta)=\frac{1}{2}||\theta||_2^2$

对单个参数$\theta_i$求导：

$$
\frac{\partial}{\partial \theta_i}(\frac{1}{2}\theta_i^2)=\theta_i
$$

这里可以看到梯度不是常数啦，和L1不同，L2的梯度和$\theta$ 有关系

梯度下降：

$$
\theta^{(t+1)} = \theta^{(t)} - \eta \big( \nabla_{\theta} L_{\text{task}} + \lambda \theta^{(t)} \big)
$$

**含义是：任务梯度 + L2 正则梯度 一起更新参数**



忽略任务梯度，只看正则项：

$$
\theta \leftarrow \theta - \eta \lambda \theta
$$

**含义是：忽略任务损失，只看L2正则项时，参数按比例衰减**



当$\theta>0$的时候，梯度=$+\lambda \theta$，往0推

当$\theta<0$的时候，梯度=$-\lambda \theta$，往0推。



趋势和L1是相同的，和L1不同的本质区别是，推的力度不同。



**L2 正则给每个参数一个“大小与自身成正比、方向指向 0”的力。**

- 参数越大 → 拉得越狠

- 参数越小 → 拉得越轻

- 参数接近 0 → 几乎不拉了

 **这就是“权重衰减（weight decay）”的本质**

L1 和 L2 都在“推参数往 0 走”，  但 L1 是“无差别地推”，会把参数推到 0 并吸住；  L2 是“按大小推”，只会让参数越来越小，但不会消失。

因为L2的特性，越接近于0，推力越小，越接近于0，越推不动了，由于训练的epoch有限，所以最后L2是越接近于0，但是不会变成0。但是L1不一样了，L1是很可能让参数变成0的。所以说L1的参数是稀疏的。



## 2 Dropout

1.dropout:在前向传播过程中，对网络中的每个隐藏层，每个隐单元都以一定的概率$P_{drop}$ 被删除，之后得到一个比原始网络要小的裁剪网络。在反向传播过程中，仅仅对该裁剪网络进行梯度更新，被删除的隐单元不进行梯度更新。

2.对隐单元的删除指的是：将该隐单元的**输出**置为0；当其输出为0时，该隐单元对后续神经元的影响均为0。注意不是把参数矩阵里面的元素置为0。而是把一个单元的输出结果置为0。一个单元的输出结果等于输入乘上参数矩阵中对应的元素。当然它的输出又是后续神经元的输入。

3.输入层和输出层的神经元不会被删除，这两层的神经元的个数是固定的。

Dropout只用于“中间隐藏层”，而不会用于输入层和输出层，因为输入层和输出层承载的是“不可替代的语义约束”，而隐藏层是“可冗余的表示”。

输入层的“神经元”本质上是：图像的元素、文本的token embedding维度或者特征向量的每一个维度。它们不是模型学出来的表示，而是数据本身的载体。如果对输入层做dropout，这是人为制造“缺失特征”。

输出层的维度是“语义绑定的”，输出层的每一个神经元通常对应：一个类别（分类）、一个具体的回归量或者一个概率分布的维度。如果对输出层做Dropout，等价于随机“删除某些类别”。

Dropout在隐藏层的作用是：强迫网络不要依赖某几个固定的神经元，而是学到更分散、更鲁棒的表示。

4.关于隐单元删除时的一些细节：

- 不同的样本，其删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的。

- 不同的样本，每个隐单元被删除的概率是相同的。

- 同一条样本，分两次进入模型训练，那么这两次删除的隐单元也是不同的。

- 在每个梯度更新周期中，被删除的隐单元的集合是不同的，因此得到的裁剪网络也是不同的。

- 在每个梯度更新周期中，隐单元被删除的概率是相同的。

- 在一次forward/backward中，如果某个隐单元被Dropout掉了，那么与它相关的那一“列/行”权重，在这一次迭代中不会更新。但这只是本次迭代，不是永久不更新。

5.dropout仅仅用于神经网络的训练截断，而推理阶段不需要删除单元，而是使用所有的神经元。

6.dropout的优点：

- 其不限神经网络的网络结构，都可以使用该技术。

- 其计算非常方便、高效；具体计算过程为：每次产生n个随机的二进制数（0和1），然后将这些产生的随机二进制数与n个隐单元的输出相乘（与0相乘的隐单元就相当于被删除了）。

7.dropout的缺点：损失函数Loss不再被明确定义，每次迭代都会随机删除一部分隐单元。

8.dropout使用时的策略：由于dropout的作用是防止过拟合，所以

- 若某一层神经元比较少，过拟合不严重，可以调小概率，甚至这层不使用dropout。

- 若某一层神经元比较多，过拟合严重，可以调大概率。

**讲一讲我自己关于dropout的拓展的理解。记得几年前看过一本书叫《失控：全人类的最终命运和结局》，这是一本上世纪的书了。当时卷积神经网络和transformer还没有出现，更不用说当前大热的大模型了。在书中作者分享了一个很有意思的事情，他观察到美国的北部的某个大草原，每过一段时间都会发生一场很大很大的火，但是之后又会有新的草长出来，逃跑的小动物又会回到这片家园。后来美国政府发现了这里的火灾，开始实施控制，扑灭火灾，结果没有过太久这个草原的生态系统崩溃了，变成了沙漠。大概故事就是这样，具体的细节可能有些出入。我想这个故事就和dropout有些相像吧，有时对一个系统不严重的破坏反而能让这个系统走向更加稳定和更加强壮。也许生态系统当中还有更多巧妙的想法可以应用到神经网络当中，应用到大模型当中，只是我现在才疏学浅，即使意识到这点，也想不到有啥可以应用的。**



## 3.数据增强（EDA）

3.1词汇增强：

- 基于词典进行替换：从文本中随机选取一个或多个词语，利用同义词词典将这一个或多个词语替换成同义词。

- 基于词向量进行替换：从文本中随机选取一个或多个词语，使用预先训练好的词向量，找到在嵌入空间中与这一个或多个词距离最近的词语，进行替换。

- 基于MLM进行替换：BERT系列模型是通过MLM进行预训练的，所以可以将随机文本中的某一个或者多个词语mask，使用预训练好的BERT系列模型对mask掉的词语进行生成，以此做增强。

- 基于tf-idf进行替换：文本中tf-id较小的词语，对该文本的贡献比较小。

- 剩下的就看不懂了，之后有机会再学习。



## 4.早停

在训练时，一般来说是训练集的损失一直下降，而验证集的损失是先下降后上升的。验证集的损失之所以上升就是因为模型已经在训练集上过拟合了。选取验证集损失最小的模型，而不是最新的模型，这种策略就是早停。
