# 优化器

## 1.梯度下降变形形式

### 1.1批量梯度下降（BGD）

批量梯度下降，计算整个训练数据集的目标函数对参数$\theta$ 的梯度：

$$
\theta=\theta-\eta\cdot\nabla_{\theta}J(\theta)
$$

批量梯度下降的意思是：每一次参数更新都是用“整个训练集”的损失函数平均梯度来更新参数。

批量梯度下降的缺点是其需要计算整个数据集的梯度，然后仅用于执行一次更新。这导致批量梯度下降速度非常慢。另外当数据量比较大的时候，内润一般也不足以加载这些数据。批量梯度下降能够保证收敛到凸函数的最小值，或者非凸函数的极小值。批量梯度下降就像是一元函数的导数一样，它用的是真实梯度值，而不是近似梯度值。

### 1.2随机梯度下降(SGD)

随机梯度下降对每个训练示例$x(i)$和标签$y(i)$执行参数更新：

$$
\theta=\theta-\eta\cdot\nabla_{\theta}J(\theta;x^{i};y^{i})
$$

随机梯度下降每次仅计算一条数据的梯度，所以SGD的速度更快。但是由于SGD每次只使用一条数据进行更新，所以其目标函数的值也会出现较大幅度的波动。

除了速度以外，SGD还有另一方面的优势。对于非凸函数，批量梯度下降会优化到极小值点，但是SGD的拨动能够使其跳出极小值点，进入更好的局部最小值点。

但是SGD的收敛过程非常复杂，甚至不能收敛到局部最小值。有一种方法是随着学习的进行慢慢降低学习率，在这种策略下SGD能够表现出与批量梯度下降较为相似的收敛行为，能够稳定收敛到凸函数的全局最小值，或者非凸函数的局部最小值。

**np.random.shuffle(data_list)**

shuffle = 在每个 epoch 开始前，把训练样本的顺序随机打乱一次。

举个例子：

- 前半个 epoch：
  
  - 模型只看到“猫”
  
  - 梯度严重偏向“猫”

- 后半个 epoch：
  
  - 模型突然只看到“狗”
  
  - 参数开始剧烈反向调整

结果是：

- 梯度方向 **剧烈震荡**

- 收敛变慢

- 泛化变差

- 甚至训练不稳定

### 1.3Mini-batch梯度下降

Mini-batch梯度下降结合批量梯度下降和SGD两者各自的优势，其每次对一个小批量的n个训练样本求梯度，并做一次更新。公式如下：

$$
\theta=\theta-\eta\cdot\nabla_{\theta}J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$

Mini-batch梯度下降的优势有：

相比于SGD，每次更新时减少了更新的方差，使得收敛比SGD更稳定。

相比于批量梯度下降，计算速度大幅提升；同时有一定的跳出局部最小值的能力。

Mini-batch / SGD 的本质：**随机梯度**每一步的方向 ≈ 正确方向 + 随机扰动

## 2.梯度下降遇到的挑战

上面提到的三种梯度下降的问题：

- 选择合适的学习率比较困难。学习率选的太小了会导致收敛缓慢，学习率选的太大了会导致不收敛，在最小值附近波动，甚至于发散。

- 有些策略是在训练的过程中调整学习率，但是这些策略里什么时候需要调整、调整为多大都是在训练前预定义好的，对不同的数据集特征的适应性比较差，换一个数据集可能就不生效了。

- 鞍点问题。很多时候问题并不是梯度下降收敛到了局部最小值，而是收敛到了鞍点。收敛到鞍点之后可能模型的效果比收敛到局部最小值还差。我们的目标是收敛到全局最小值。
  
  **鞍点**：在某些方向上是“最低点”（像谷底）；在另一些方向上是“最高点”（像山脊）；**梯度为 0，但既不是最小值，也不是最大值**。
  
  鞍点是梯度为零但既非局部最小也非局部最大的位置，其 Hessian 矩阵同时具有正负特征值。在高维非凸优化问题中，鞍点比局部最小值更常见，且更容易导致梯度下降停滞，从而影响模型性能。

Hessian 矩阵是：**“二阶导数的矩阵”**

## 3.梯度下降优化算法

梯度下降优化算法的共同之处：**一般是求一阶动量(m)和二阶动量(v),然后利用一阶、二阶动量本身或者他们的组合来优化梯度下降**（其中一阶动量为与梯度相关的函数，二阶动量为与梯度平方相关的函数。）

一阶动量是“最近一段时间内，梯度的平均方向与强度”。

二阶动量是”最近一段时间内，梯度的幅值的平均大小“。因为二阶是没有方向的。

梯度下降的公式：

$$
\theta_{t+1}=\theta_t-\eta \frac{\partial (Loss)}{\partial \theta_t}
$$

引入梯度下降优化算法后：

$$
\theta_{t+1} = \theta_t - \eta \frac{m_t}{\sqrt{V_t}}
$$

### 3.1动量法

随机梯度下降的方法很难通过峡谷区域（即在一个维度梯度变化很大，另一个维度变化较小），梯度下降沿着负梯度方向更新参数，该方向对应当前点处函数下降最快的方向。但当不同维度的曲率差异较大时，统一学习率会导致某些方向更新过大产生振荡，而另一些方向更新过慢。

峡谷不是由“梯度大”定义的，而是由“梯度变化速度不同”定义的。

动量法可以解决峡谷问题，能够对朝着极值点的方向加速，并能够一定程度上抑制震荡。具体做法是：**将过去的时间步更新的向量的一部分$\gamma$添加到当前更新的向量上**。通过这种方式，历史时间步中使得优化轨迹来回震荡的方向的动量会相互抵消，剩下的主要是朝着极值点的方向的动量。公式如下所示：

$$
m_t=\gamma m_{t-1}+\eta \nabla_{\theta}J(\theta)
$$

$$
\theta_{t+1}=\theta_{t}-m_t
$$

动量项$\gamma$ 通常设置为0.9或者类似值。

本质上，当使用动量法时：对于梯度指向相同方向的维度，动量项会增加，而对于梯度改变方向的维度，动量项的更新会减少。我们获得了更快的收敛速度并减少了振荡。

### 3.2NAG

公式如下：

$$
m_t=\gamma m_{t-1}+\eta \nabla_{\theta}J(\theta-\gamma m_{t-1})
$$

$$
\theta_{t+1}=\theta_{t}-m_t
$$

NAG的直觉是：先按照动量走一走，再在那个“预测位置”上看梯度。

这个例子比较困难，这里要认真看一看。

一定要明白这里面的这些参数的含义，并且了解它们的作用。

$\theta_t$代表当前模型的参数，可以理解为你现在站的位置。

$\theta_{t+1}$代表模型新参数，可以理解为更新后的真实位置。

$m_t$代表动量/速度，可以理解为历史梯度的累积。

$\gamma$代表动量系数，可以理解为旧速度保留比例。

$\eta$代表学习率，可以理解为新梯度影响强度。

$\nabla J(\cdot)$代表梯度，可以理解为局部最陡方向。

$\theta_t-\gamma m_{t-1}$ 代表前瞻点，可以理解为惯性预测位置，如果你只按照当前动量继续走，马上会到的地方。

Nesterov的逻辑不是公式，而是这段话：

我现在在$\theta_t$,

我已经有了一股惯性$m_{t-1}$，

如果我继续冲，我大概会到$\theta_t-\gamma m_{t-1}$，

那我先在那儿看一眼坡度，

再决定要不要修正速度（这里这样说是不对的，应该是在前瞻位置计算梯度后，根据该梯度的大小和方向，自然确定对当前动量的修正幅度。梯度大->修正幅度大。梯度小->修正幅度小。梯度反向->强烈的减速或者掉头了。），

最后真正更新到$\theta_{t+1}$。

梯度反向不是偶然的，  它只能发生在你已经穿过（或即将穿过）最优区域之后。所以当我们看到接下来如果要梯度反向的话，我们要重视，因为它代表你已经冲过头了，或者即将冲过头了。

优化器不是在追求“永远不要出现反向梯度”，  而是在利用反向梯度作为“减速与纠偏的刹车信号”，  从而避免由于惯性导致的震荡和过冲。

### 3.3 Adagrad

之前提到的几个方法当中，对于所有特征我们的学习率一直没有变化。Adagrad算法就是为了解决这个问题，让学习率学习数据的特征自动调整其大小，Adagrad算法引入了二阶动量（V），其表达式为：

$$
m_t=g_t\\
V_t=\sum_\tau^t g_\tau^2 \\
\theta_{t+1}=\theta_t-\eta\frac {m_t}{\sqrt V_t}
$$

其中$g(t)$为$t$时刻参数梯度。

- AdaGrad **没有一阶动量**

- 更新方向直接用当前梯度

下面讲解为什么adagrad可以实现不同频率特征对其参数学习率改变。

首先，看到二阶动量$V(t)$，它是梯度平方累加和，对于训练数据少的特征，自然对应的参数更新就缓慢，也就是说它们的梯度变化平方累加和就会比较小，所以对应上面参数更新方程中的学习速率就会变大，所以对于某个特征数据集少，相应参数更新速度就快。为了防止上述分母变成0，所以往往会添加一个平滑项参数$\epsilon$，参数更新方差就变成了：

$$
\theta_{t+1}=
\theta_t-\eta\frac {m_t}{\sqrt {V_t+\epsilon}}
$$

但是Adagrad同样也有问题，就是其分母随着训练数增加，也会跟着增加，这样会导致学习速率越来越小，最终变的无限小，从而无法有效更新参数。

AdaGrad中二阶动量$V(t)$的标准定义是什么呢？

在AdaGrad中，二阶动量（更准确说是”累积平方梯度“）定义为：

$$
V(t)=\sum_{i=1}^t g_i \odot g_i
$$

其中：$g_i=\nabla_\theta J(\theta_i)$表示第$i$次迭代的梯度。

$\odot$:表示逐元素平方。

如果按照单个参数维度$j$来写，会更清楚：

$$
V_j(t)=\sum_{i=1}^t(g_{i,j})^2
$$

所以对于训练数据少的特征，自然对应的参数的梯度累计增长缓慢，该特征很少被激活，对应的梯度经常为0。所以$V_j(t)$增长很慢。所以说它们的梯度变化平方和累加和就会比较小。所以对应上面的参数更新方程中的学习速率就会变大。所以对于某个特征数据集少，相应参数更新速度就快。

大家常说的某个特征的梯度很小或者为0是什么意思呢？

梯度永远是对参数$\theta_j$求的，不是对特征$x_j$

举个例子：$y=w^Tx$

参数：$wj$

特征：$x_j$

损失对参数的梯度是：$\frac{\partial (Loss)}{\partial w_j}=(误差)·x_j$

如果某个特征：

$x_j=0$,或者有很少是非0 的。

那么$\frac{\partial (Loss)}{\partial \theta_t}\approx0$

所以当特征为0时，它对应的梯度经常为0。

# 3.4 Adam

Adam在实际中表现很好，同时与其他自适应学习算法相比更有优势。

Adam中文名为自适应矩估计，是另一种自适应学习率的算法，它是一种将动量和Adagrad结合起来的算法，也就引入了两个参数$\beta_1$和$\beta_2$,其一阶和二阶动量公式为：

$$
g_t=\nabla_{\theta}J(\theta_t)\\
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t\\
V_t=\beta_2V_{t-1}+(1-\beta_2)g_t^2

$$

参数更新：

$$
\theta_{t+1}=
\theta_t-\eta\frac {m_t}{\sqrt {V_t+\epsilon}}
$$

Adam = **Momentum（方向更稳）** + **RMSProp（步长自适应）**

- 用 **一阶动量** $m_t$平滑梯度（像“速度/趋势”）

- 用 **二阶动量** $v_t$​ 估计梯度幅值尺度（像“每个维度的刹车系数”）

- 用 $\frac{m_t}{\sqrt{v_t}}$ 让不同维度拥有不同“有效学习率”

## 为什么要做 bias correction（偏置修正）

因为初始化时通常设：$m_0=0,v_0=0$

EMA 在前几步会被“拉向 0”，导致 $m_t​,v_t$​ **系统性偏小**。Adam 用偏置修正得到更接近真实矩估计的量：

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \qquad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}

$$

直觉：前期 EMA 还没“热起来”，除以 $1-\beta_1^t$相当于把它拉回合理尺度。(证明比较复杂，暂时就不证明了。)

很多深度学习（尤其 Transformer/LLM）里说“用 Adam”，实际通常是 **AdamW**：把 **weight decay（权重衰减）** 从梯度里“解耦”出来。


